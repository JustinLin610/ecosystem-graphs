---
- type: model
  name: InternVideo
  organization: Shanghai AI Laboratory
  description: ''
  created_date: 2022-12-06
  url: https://arxiv.org/pdf/2212.03191.pdf
  model_card: ''
  modality: text, video; video
  analysis: ''
  size: 1.3B parameters (dense)
  dependencies:
    - Kinetics-400
    - WebVid-2M
    - WebVid-10M
    - HowTo100M
    - AVA
    - Something-Something-v2
    - Kinetics-710
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: open
  license: Apache 2.0
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: Lego-MT
  organization: Shanghai AI Laboratory
  description: Lego-MT is a multilingual large language model which uses a more
    efficient approach of being an effective detachable model.
  created_date: 2023-05-29
  url: https://arxiv.org/pdf/2212.10551.pdf
  model_card: ''
  modality: text; text
  analysis: Evaluated based on own constructed dataset covering 433 languages.
  size: 1.2B parameters (dense)
  dependencies: [OPUS]
  training_emissions: unknown
  training_time: 15 days
  training_hardware: 32 A100 GPUs
  quality_control: ''
  access: open
  license: ''
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''

- type: model
  name: MathCoder
  organization: Shanghai AI Laboratory
  description: MathCoder is a family of models capable of generating code-based
    solutions for solving challenging math problems.
  created_date: 2023-10-05
  url: https://arxiv.org/pdf/2310.03731.pdf
  model_card: none
  modality: text; text
  analysis: Evaluated on GSM8K and the competition-level MATH dataset.
  size: 70B parameters (dense)
  dependencies: [GPT-4, LLaMA 2]
  training_emissions: unknown
  training_time: unknown
  training_hardware: 32 NVIDIA A800 80GB GPUs
  quality_control: none
  access: open
  license: unknown
  intended_uses: bridging the gap between natural language understanding and computational
    problem-solving
  prohibited_uses: none
  monitoring: none
  feedback: none
- type: model
  name: InternLM
  organization: Shanghai AI Laboratory
  description: InternLM is a high-quality language model proficient in English,
    Chinese, and code.
  created_date: 2023-09-20
  url: https://github.com/InternLM/InternLM
  model_card: https://huggingface.co/internlm/internlm-20b
  modality: code, text; code, text
  analysis: Evaluated in comparison to LLaMA series models on standard benchmarks.
  size: 20B parameters (dense)
  dependencies: []
  training_emissions: unknown
  training_time: unknown
  training_hardware: unknown
  quality_control: ''
  access: open
  license: Apache 2.0
  intended_uses: ''
  prohibited_uses: ''
  monitoring: none
  feedback: https://huggingface.co/internlm/internlm-20b/discussions
