---
- type: dataset
  name: Wu Dao dataset
  organization: Beijing Academy of Artificial Intelligence
  description: ''
  created_date:
    explanation: "The date that BAAI made a public announcement [[News Link]](https://mp.weixin.qq.com/s/BUQWZ5EdR19i40GuFofpBg).\n"
    value: 2021-01-12
  url: https://www.tsinghua.edu.cn/en/info/1420/10473.htm
  datasheet: ''
  modality: image, text
  size: unknown
  sample: []
  analysis: ''
  dependencies: []
  included: ''
  excluded: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: Wu Dao 2.0
  organization: Beijing Academy of Artificial Intelligence
  description: ''
  created_date:
    explanation: "The date that BAAI made a public announcement [[News Link]](https://mp.weixin.qq.com/s/BUQWZ5EdR19i40GuFofpBg).\n"
    value: 2021-01-12
  url: https://www.tsinghua.edu.cn/en/info/1420/10473.htm
  model_card: ''
  modality: image, text
  analysis: ''
  size: 1.75T parameters (dense)
  dependencies: [Wu Dao dataset]
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: JudgeLM
  organization: Beijing Academy of Artificial Intelligence
  description: JudgeLM is a fine-tuned to be a scalable judge to evaluate LLMs efficiently
    and effectively in open-ended benchmarks.
  created_date: 2023-10-26
  url: https://arxiv.org/pdf/2310.17631.pdf
  model_card: https://huggingface.co/BAAI/JudgeLM-13B-v1.0
  modality: text; text
  analysis: Evaluated on objective and reliability metrics.
  size: 13B parameters (dense)
  dependencies: [Vicuna, JudgeLM Dataset]
  training_emissions: unknown
  training_time: unknown
  training_hardware: 8 A100 40GB NVIDIA GPUs
  quality_control: ''
  access: open
  license: Apache 2.0
  intended_uses: Research on evaluating the performance of large language models
    and chatbots.
  prohibited_uses: none
  monitoring: none
  feedback: https://huggingface.co/BAAI/JudgeLM-13B-v1.0/discussions
- type: dataset
  name: JudgeLM Dataset
  organization: Beijing Academy of Artificial Intelligence
  description: JudgeLM Dataset is a novel dataset replete with a rich variety of
    seed tasks, comprehensive answers from modern LLMs, answersâ€™ grades from the
    teacher judge, and detailed reasons for judgments.
  created_date: 2023-10-26
  url: https://huggingface.co/datasets/BAAI/JudgeLM-100K
  datasheet: ''
  modality: text, text
  size: 105k judge samples
  sample: []
  analysis: none
  dependencies: [Alpaca, GPT-4, Dolly, ShareGPT, LLaMA, Vicuna]
  included: ''
  excluded: ''
  quality_control: ''
  access: open
  license: CC BY NC 4.0
  intended_uses: To be used to conduct instruction-tuning for language models and
    make the language model able to judge open-ended answer pairs.
  prohibited_uses: none
  monitoring: none
  feedback: https://huggingface.co/datasets/BAAI/JudgeLM-100K/discussions

- type: model
  name: SegMamba
  organization: Hong Kong University of Science and Technology (Guangzhou + original),
    Beijing Academy of Artificial Intelligence
  description: SegMamba is a novel 3D medical image Segmentation Mamba model, designed
    to effectively capture long-range dependencies within whole volume features
    at every scale.
  created_date: 2024-01-25
  url: https://arxiv.org/pdf/2401.13560v2.pdf
  model_card: none
  modality: image; text
  analysis: Compared to other segmentation models across different modalities on
    BraTS2023 dataset.
  size: unknown
  dependencies: []
  training_emissions: unknown
  training_time: 1000 epochs
  training_hardware: 4 NVIDIA A100 GPUs
  quality_control: unknown
  access: open
  license: Apache 2.0
  intended_uses: ''
  prohibited_uses: ''
  monitoring: unknown
  feedback: none

- type: model
  name: BGE M3 Embedding
  organization: Beijing Academy of Artificial Intelligence, University of Science
    and Technology of China
  description: BGE M3 Embedding is a new embedding model that can support more than
    100 working languages, leading to new state-of-the-art performances on multi-lingual
    and cross-lingual retrieval tasks.
  created_date: 2024-02-05
  url: https://arxiv.org/pdf/2402.03216.pdf
  model_card: https://huggingface.co/BAAI/bge-m3
  modality: text; text
  analysis: Evaluated on standard datasets in multilingual, cross-lingual, long
    document retrieval, and Q&A domains.
  size: unknown
  dependencies: []
  training_emissions: unknown
  training_time: 20,000 steps
  training_hardware: 32 A100 40GB GPUs
  quality_control: unknown
  access: open
  license: MIT
  intended_uses: ''
  prohibited_uses: ''
  monitoring: unknown
  feedback: https://huggingface.co/BAAI/bge-m3/discussions
