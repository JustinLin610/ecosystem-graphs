---
- type: model
  name: Guanaco
  organization: University of Washington
  description: Guanaco is a model family trained with QLORA, an efficient finetuning
    approach that reduces memory usage enough to finetune a 65B parameter model
    on a single 48GB GPU while preserving full 16-bit finetuning task performance.
  created_date: 2023-05-23
  url: https://arxiv.org/pdf/2305.14314v1.pdf
  model_card: ''
  modality: text; text
  analysis: Reports results on the Vicuna benchmark and compares performance level
    and time expenditure with ChatGPT
  size: 33B parameters (dense)
  dependencies: [QLoRA, OASST1]
  training_emissions: ''
  training_time: ''
  training_hardware: A single 24 GB GPU
  quality_control: ''
  access: open
  license: MIT
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: Llark
  organization: University of Washington, Spotify
  description: Llark is an instruction-tuned multimodal model for music understanding.
  created_date: 2023-10-11
  url: https://arxiv.org/pdf/2310.07160.pdf
  model_card: none
  modality: audio, text; text
  analysis: Evaluated on benchmark music understanding tasks on SOTA music datasets.
  size: 12B parameters (dense)
  dependencies: [LLaMA 2, Jukebox]
  training_emissions: unknown
  training_time: 54 hours
  training_hardware: 4 80GB NVIDIA A40 GPUs
  quality_control: ''
  access: open
  license: Apache 2.0
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: none
